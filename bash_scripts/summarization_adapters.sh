experiment_name = "t5_base_cnn_dailymail_pfeiffer_adapters"
python scripts/finetuning_seq2seq.py \
    --output_dir "results/$experiment_name"\
    --seed 0 \
    --model_name_or_path "t5-base" \
    --dataset_name "cnn_dailymail" \
    --dataset_config_name "3.0.0" \
    --peft_library "adapter-transformers" \
    --adapter_config_string "pfeiffer" \
    --per_device_train_batch_size 8 \
    --per_device_eval_batch_size 16 \
    --gradient_accumulation_steps 4 \
    --use_quantization false \
    --load_in_4bit false \
    --max_source_length 1024 \
    --max_target_length 128 \
    --num_beams 5 \
    --learning_rate 0.0001 \
    --lr_scheduler_type "linear" \
    --lr_scheduler_warmup_percent 0.06 \
    --weight_decay 0 \
    --num_train_epochs 1 \
    --eval_every_steps 1000 \
    --wandb_project "PEFT_comparison" \
    --source_prefix "summarize: "
