{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (AutoModel, AutoTokenizer, LlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model_class = LlamaForCausalLM\n",
    "model = model_class.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"Our friends won't buy this analysis, let alone the next one we propose. Is it an acceptable sentence?\"#\"Sentence: I feel wonderful today. Sentiment:\"\n",
    "target_text = \"acceptable\"#\"positive\"\n",
    "\n",
    "batch = {\n",
    "    \"input_ids\": tokenizer.encode_plus(source_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device),\n",
    "    \"labels\": tokenizer.encode_plus(target_text, return_tensors=\"pt\")[\"input_ids\"].to(model.device),\n",
    "}\n",
    "\n",
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = model.generate(\n",
    "    input_ids=batch[\"input_ids\"],\n",
    "    #attention_mask=batch[\"attention_mask\"],\n",
    "    #max_length=batch[\"input_ids\"].__len__(),\n",
    "    num_beams=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.batch_decode(\n",
    "    generated_tokens,\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long_seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
