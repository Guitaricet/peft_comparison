{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model,\n",
    "    TaskType, \n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"t5-small\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(\n",
       "  in_features=512, out_features=512, bias=False\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict(\n",
       "    (default): Linear(in_features=512, out_features=16, bias=False)\n",
       "  )\n",
       "  (lora_B): ModuleDict(\n",
       "    (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "  )\n",
       "  (lora_embedding_A): ParameterDict()\n",
       "  (lora_embedding_B): ParameterDict()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.base_model.model.encoder.block[0].layer[0].SelfAttention.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0162, -0.0737,  0.0042,  ...,  0.0271, -0.0366, -0.1191],\n",
       "        [-0.0371, -0.0227,  0.0491,  ..., -0.0427, -0.0486,  0.0669],\n",
       "        [ 0.0540, -0.0566,  0.0054,  ..., -0.0339, -0.0173, -0.0430],\n",
       "        ...,\n",
       "        [-0.0908,  0.0104, -0.1104,  ..., -0.0070, -0.1001,  0.0405],\n",
       "        [ 0.0017,  0.0157, -0.0427,  ..., -0.0962,  0.0300,  0.0015],\n",
       "        [-0.1069,  0.0002, -0.0029,  ...,  0.0530, -0.0432, -0.0522]],\n",
       "       device='cuda:1', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.base_model.model.encoder.block[0].layer[0].SelfAttention.q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.base_model.model.encoder.block[0].layer[0].SelfAttention.q.lora_A.default.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_conparison",
   "language": "python",
   "name": "peft_conparison"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
