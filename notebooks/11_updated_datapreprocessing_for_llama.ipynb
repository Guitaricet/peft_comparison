{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pprint import pformat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm, trange\n",
    "from loguru import logger\n",
    "\n",
    "import scripts\n",
    "from adapters.models.llama.adapter_model import LlamaAdapterModel\n",
    "import peft_comparison\n",
    "import peft_comparison.text2text_utils\n",
    "import peft_comparison.mappings\n",
    "#from peft_comparison.collation import DataCollatorForSeq2SeqWithMetadata, DataCollatorForCausalLMWithMetadata\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = \"max_length\"\n",
    "truncation = True\n",
    "\n",
    "source_prefix = \"\"\n",
    "max_source_length = 512\n",
    "decoder_only = True\n",
    "max_target_length = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.add_special_tokens({'pad_token': \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we tokenize all the texts.\n",
    "def preprocess_function(examples, is_eval=False, decoder_only=False):\n",
    "        inputs = examples[\"source_text\"]\n",
    "        targets = examples[\"target_text\"]\n",
    "        inputs = [source_prefix + inp for inp in inputs]\n",
    "\n",
    "        if not decoder_only:\n",
    "            # T5\n",
    "            model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "            labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "            if padding == \"max_length\":\n",
    "                labels[\"input_ids\"] = [\n",
    "                    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "                ]\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            if is_eval:\n",
    "                model_inputs[\"metadata\"] = [{\"targets\": t} for t in targets]\n",
    "\n",
    "        else:\n",
    "            # @NOTE: the way we have written preprocessing and collation for llama,\n",
    "            # - set padding=False in preprocessing (so that we know what's the max_len in the batch)\n",
    "            # - set padding=True in collation (so that we can pad to the multiple of 8 > max_len in the batch)\n",
    "            # - we can set labels to input_ids because the token shifting is taken care of in the modeling_llaama file\n",
    "            if is_eval:\n",
    "                model_inputs = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=True)\n",
    "            else:\n",
    "                inputs = [i + \" \" + t for i, t in zip(inputs, targets)]\n",
    "                model_inputs = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=True)\n",
    "            model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "            if is_eval:\n",
    "                input_wo_label = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=False)\n",
    "                input_wo_label = input_wo_label[\"input_ids\"]\n",
    "                model_inputs[\"metadata\"] = []\n",
    "                for idx in range(len(targets)):\n",
    "                    model_inputs[\"metadata\"].append(\n",
    "                        {\n",
    "                            \"targets\": targets[idx],\n",
    "                            \"input_len\": len(input_wo_label[idx]),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausalLMWithMetadata:\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "    padding_side: str = \"left\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        return_tensors = self.return_tensors\n",
    "        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0].keys() else None\n",
    "        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
    "        # same length to return tensors.\n",
    "        max_label_length = max(len(l) for l in labels)\n",
    "        if self.pad_to_multiple_of is not None:\n",
    "            max_label_length = (\n",
    "                (max_label_length + self.pad_to_multiple_of - 1)\n",
    "                // self.pad_to_multiple_of\n",
    "                * self.pad_to_multiple_of\n",
    "            )\n",
    "\n",
    "        batch = {}\n",
    "        for feature in features:\n",
    "            for k, v in feature.items():\n",
    "                if k == \"metadata\": continue\n",
    "                if k not in batch:\n",
    "                    batch[k] = []\n",
    "\n",
    "                # fill the sequence upto the \"max_label_length\" with appropriate token_id (either eos or 0)\n",
    "                if k in [\"input_ids\"]:\n",
    "                    remainder = [self.tokenizer.pad_token_id] * (max_label_length - len(v))\n",
    "                elif k in [\"labels\"]:\n",
    "                    remainder = [self.label_pad_token_id] * (max_label_length - len(v))\n",
    "                elif k in [\"attention_mask\", \"decoder_attention_mask\"]:\n",
    "                    remainder = [0] * (max_label_length - len(v))\n",
    "                else:\n",
    "                    ValueError(f\"Invalid key {k}\")\n",
    "\n",
    "                # padding: either to the right or left\n",
    "                if self.padding_side == \"right\":\n",
    "                    v = v + remainder\n",
    "                else:\n",
    "                    v = remainder + v\n",
    "\n",
    "                assert len(v) == max_label_length, f\"len(v)={len(v)}, max_label_length={max_label_length}\"\n",
    "                batch[k].append(v)\n",
    "\n",
    "        # convert values to torch tensors\n",
    "        batch = {k: torch.LongTensor(v) for k, v in batch.items()}\n",
    "        if \"metadata\" in features[0].keys():\n",
    "            batch[\"metadata\"] = [feature[\"metadata\"] for feature in features]\n",
    "\n",
    "        \"\"\"\n",
    "        if labels is not None:\n",
    "            max_label_length = max(len(l) for l in labels)\n",
    "            if self.pad_to_multiple_of is not None:\n",
    "                max_label_length = (\n",
    "                    (max_label_length + self.pad_to_multiple_of - 1)\n",
    "                    // self.pad_to_multiple_of\n",
    "                    * self.pad_to_multiple_of\n",
    "                )\n",
    "\n",
    "            padding_side = self.padding_side\n",
    "            for feature in features:\n",
    "                remainder = [self.tokenizer.eos_token_id] * (max_label_length - len(feature[\"labels\"])) #[self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n",
    "                if isinstance(feature[\"labels\"], list):\n",
    "                    if padding_side == \"right\":\n",
    "                        feature[\"labels\"] = feature[\"labels\"] + remainder\n",
    "                    else:\n",
    "                        print(\"here\")\n",
    "                        feature[\"labels\"] = remainder + feature[\"labels\"]\n",
    "                    feature[\"labels\"] = (\n",
    "                        feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
    "                    )\n",
    "\n",
    "                elif padding_side == \"right\":\n",
    "                    feature[\"labels\"] = np.concatenate([feature[\"labels\"], remainder]).astype(np.int64)\n",
    "                else:\n",
    "                    feature[\"labels\"] = np.concatenate([remainder, feature[\"labels\"]]).astype(np.int64)\n",
    "\n",
    "        #non_str_features = [\n",
    "        #    {k: v for k, v in feature.items() if k != \"metadata\"} for feature in features\n",
    "        #]\n",
    "\n",
    "        non_str_features = {}\n",
    "        for feature in features:\n",
    "            for k, v in feature.items():\n",
    "                if k != \"metadata\":\n",
    "                    if k not in non_str_features:\n",
    "                        non_str_features[k] = []\n",
    "                        print(len(v))\n",
    "                    non_str_features[k].append(v)\n",
    "        non_str_features = {k: torch.LongTensor(v) for k, v in non_str_features.items()}\n",
    "        non_str_features = self.tokenizer.pad(\n",
    "            non_str_features,\n",
    "            truncation=True,\n",
    "            padding=False,#self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=return_tensors,\n",
    "        )\n",
    "\n",
    "        #\n",
    "        if \"metadata\" in features[0].keys():\n",
    "            non_str_features[\"metadata\"] = [feature[\"metadata\"] for feature in features]\n",
    "        features = non_str_features\n",
    "        \"\"\"\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"super_glue\", \"copa\")\n",
    "raw_datasets, postprocess_fn = peft_comparison.text2text_utils.dataset_to_text2text(\n",
    "    raw_datasets,\n",
    "    task_type=\"classification\",\n",
    "    dataset_name=\"copa\",\n",
    "    decoder_only=True,\n",
    ")\n",
    "column_names = raw_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'My body cast a shadow over the grass.',\n",
       " 'choice1': 'The sun was rising.',\n",
       " 'choice2': 'The grass was cut.',\n",
       " 'question': 'cause',\n",
       " 'idx': 0,\n",
       " 'label': 0,\n",
       " 'source_text': 'Given a premise, a question (cause/effect) and two alternative choices, identify plausible answer from the alternative choices.  premise: My body cast a shadow over the grass. question: cause choice1: The sun was rising. choice2: The grass was cut. Select answer from: choice1,choice2. Answer:',\n",
       " 'target_text': 'choice1'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on val dataset  \",\n",
    "    fn_kwargs={\"is_eval\": True, \"decoder_only\": decoder_only},\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=min(5000, len(raw_datasets[\"train\"]) // 8),\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    "    fn_kwargs={\"decoder_only\": decoder_only},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pad_token_id = -100\n",
    "data_collator = DataCollatorForCausalLMWithMetadata(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,\n",
    "    max_length=max_source_length,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=2)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 88])\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "tensor([ 5, 12])\n",
      "Source: </s></s></s></s></s><s> Given a premise, a question (cause/effect) and two alternative choices, identify plausible answer from the alternative choices.  premise: The boy mimicked his older brother. question: cause choice1: The boy looked up to his older brother. choice2: The boy wrestled with his older brother. Select answer from: choice1,choice2. Answer: choice1\n",
      "Source: </s></s></s></s></s></s></s></s></s></s></s></s><s> Given a premise, a question (cause/effect) and two alternative choices, identify plausible answer from the alternative choices.  premise: The service at the restaurant was slow. question: cause choice1: There were many empty tables. choice2: The restaurant was crowded. Select answer from: choice1,choice2. Answer: choice2\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    print(batch.keys())\n",
    "    print((~(batch[\"attention_mask\"].bool())).sum(dim=1))\n",
    "    s_ = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "    #t_ = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=False)\n",
    "    for idx, ex in enumerate(s_):\n",
    "        print(f\"Source: {ex}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 88])\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'metadata'])\n",
      "tensor([9, 7])\n",
      "Source: </s></s></s></s></s></s></s></s></s><s> Given a premise, a question (cause/effect) and two alternative choices, identify plausible answer from the alternative choices.  premise: The man turned on the faucet. question: effect choice1: The toilet filled with water. choice2: Water flowed from the spout. Select answer from: choice1,choice2. Answer:\n",
      "Source: </s></s></s></s></s></s></s><s> Given a premise, a question (cause/effect) and two alternative choices, identify plausible answer from the alternative choices.  premise: The girl found a bug in her cereal. question: effect choice1: She poured milk in the bowl. choice2: She lost her appetite. Select answer from: choice1,choice2. Answer:\n"
     ]
    }
   ],
   "source": [
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    print(batch.keys())\n",
    "    print((~(batch[\"attention_mask\"].bool())).sum(dim=1))\n",
    "    s_ = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "    #t_ = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=False)\n",
    "    for idx, ex in enumerate(s_):\n",
    "        print(f\"Source: {ex}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_ids\"][0, s_:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in eval_dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    t_ = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "    for ex in t_:\n",
    "        print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_comparison_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
