{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import adapters\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoConfig\n",
    "from adapters import LlamaAdapterModel\n",
    "from transformers import AutoTokenizer\n",
    "from adapters.models.llama.adapter_model import LlamaAdapterModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1b\")\n",
    "\n",
    "#\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "#config.load_in_8bit = True\n",
    "\n",
    "#\n",
    "model = LlamaAdapterModel.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_4bit=True)\n",
    "model.add_causal_lm_head(\"lm_head\")\n",
    "\n",
    "#\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_4bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAdapterModel(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayerWithAdapters(\n",
       "        (self_attn): LlamaAttentionWithAdapters(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (prefix_tuning): PrefixTuningShim(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (attention_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (lm_head): CausalLMHead(\n",
       "      (0): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.heads.lm_head[0].weight = model_ref.lm_head.weight\n",
    "del model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_ref\u001b[39m.\u001b[39mlm_head\u001b[39m.\u001b[39mweight\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ref' is not defined"
     ]
    }
   ],
   "source": [
    "model_ref.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for name, par in model.named_parameters():\n",
    "    par.requires_grad = False\n",
    "\n",
    "#model.add_adapter(\"adapter\", config=\"lora\", set_active=True)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAdapterModel(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayerWithAdapters(\n",
       "        (self_attn): LlamaAttentionWithAdapters(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (prefix_tuning): PrefixTuningShim(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (attention_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (lm_head): CausalLMHead(\n",
       "      (0): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdapterLayer(\n",
       "  (adapters): ModuleDict()\n",
       "  (adapter_fusion_layer): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].attention_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters.init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 33])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "toked = tokenizer.encode_plus(\n",
    "    \"Sentence: I feel good. Sentiment of the previous sentence: Positive. Sentence: I feel bad. Sentiment of the previous sentence: \", return_tensors=\"pt\")\n",
    "input_ids = toked[\"input_ids\"]\n",
    "attn_mask = toked[\"attention_mask\"]\n",
    "input_ids = input_ids.to(device)\n",
    "attn_mask = attn_mask.to(device)\n",
    "print(input_ids.shape)\n",
    "print(input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_length=100,\n",
    "    min_length=2,\n",
    "    num_beams=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(\"adapter\", \"pfeiffer\", set_active=True)\n",
    "model.train()\n",
    "model.train_adapter(\"adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].output_adapters.adapters.adapter.adapter_down[0].weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model when it is dispatched on multiple devices.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/accelerate/big_modeling.py:416\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mdevice \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    415\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/transformers/modeling_utils.py:2053\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[39m@wraps\u001b[39m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule\u001b[39m.\u001b[39mto)\n\u001b[1;32m   2050\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2051\u001b[0m     \u001b[39m# Checks if the model has been loaded in 8-bit\u001b[39;00m\n\u001b[1;32m   2052\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantization_method\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mBITS_AND_BYTES:\n\u001b[0;32m-> 2053\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2054\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2055\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2056\u001b[0m         )\n\u001b[1;32m   2057\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2058\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"adapter\" in name:\n",
    "        module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].output_adapters.adapters.adapter.adapter_down[0].weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Half and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model(input_ids\u001b[39m=\u001b[39;49minput_ids)\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m hook_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/models/llama/adapter_model.py:71\u001b[0m, in \u001b[0;36mLlamaAdapterModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, head, output_adapter_gating_scores, output_adapter_fusion_attentions, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m     67\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m---> 71\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m     72\u001b[0m     input_ids,\n\u001b[1;32m     73\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m     74\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     75\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m     76\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m     77\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m     78\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m     79\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m     80\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m     81\u001b[0m     output_adapter_gating_scores\u001b[39m=\u001b[39;49moutput_adapter_gating_scores,\n\u001b[1;32m     82\u001b[0m     output_adapter_fusion_attentions\u001b[39m=\u001b[39;49moutput_adapter_fusion_attentions,\n\u001b[1;32m     83\u001b[0m     adapter_input_parallelized\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39madapter_input_parallelized\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     86\u001b[0m batch_size \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[39m# TODO-AH: this may result in unexpected behavior for classification. Find a better way to do this?\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m hook_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/context.py:108\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m ctx:\n\u001b[1;32m    105\u001b[0m     kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    106\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39moutput_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcontext_attributes\n\u001b[1;32m    107\u001b[0m     }\n\u001b[0;32m--> 108\u001b[0m     results \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    110\u001b[0m     \u001b[39m# append output attributes\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/model_mixin.py:1230\u001b[0m, in \u001b[0;36mModelBaseAdaptersMixin.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[39m@ForwardContext\u001b[39m\u001b[39m.\u001b[39mwrap\n\u001b[1;32m   1229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1230\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[1;32m    704\u001b[0m         attention_mask,\n\u001b[1;32m    705\u001b[0m         position_ids,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    709\u001b[0m         hidden_states,\n\u001b[1;32m    710\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    711\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    712\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    713\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    714\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    717\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m hook_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/models/llama/modeling_llama.py:160\u001b[0m, in \u001b[0;36mLlamaDecoderLayerWithAdapters.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    158\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    159\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m--> 160\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_adapters(hidden_states, residual, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    162\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m hook_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/layer.py:702\u001b[0m, in \u001b[0;36mAdapterLayer.forward\u001b[0;34m(self, hidden_states, residual_input, layer_norm)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, residual_input, layer_norm):\n\u001b[1;32m    692\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward pass through the adapter layer.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \n\u001b[1;32m    694\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39m        torch.Tensor: Output hidden states of the adapter layer.\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madapter_layer_forward(hidden_states, residual_input, layer_norm)\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/layer.py:661\u001b[0m, in \u001b[0;36mAdapterLayer.adapter_layer_forward\u001b[0;34m(self, hidden_states, residual_input, layer_norm)\u001b[0m\n\u001b[1;32m    658\u001b[0m input_hidden_states \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(adapter_setup, Stack):\n\u001b[0;32m--> 661\u001b[0m     hidden_states, _, residual_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madapter_stack(\n\u001b[1;32m    662\u001b[0m         adapter_setup, hidden_states, residual_input, layer_norm\n\u001b[1;32m    663\u001b[0m     )\n\u001b[1;32m    664\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(adapter_setup, Fuse):\n\u001b[1;32m    665\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter_fusion(adapter_setup, hidden_states, residual_input, layer_norm)\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/layer.py:284\u001b[0m, in \u001b[0;36mAdapterLayer.adapter_stack\u001b[0;34m(self, adapter_setup, hidden_states, input_tensor, layer_norm, lvl)\u001b[0m\n\u001b[1;32m    282\u001b[0m hidden_states, _, residual \u001b[39m=\u001b[39m adapter_layer\u001b[39m.\u001b[39mpre_forward(hidden_states, input_tensor, layer_norm)\n\u001b[1;32m    283\u001b[0m context \u001b[39m=\u001b[39m ForwardContext\u001b[39m.\u001b[39mget_context()\n\u001b[0;32m--> 284\u001b[0m layer_output \u001b[39m=\u001b[39m adapter_layer(\n\u001b[1;32m    285\u001b[0m     hidden_states, residual_input\u001b[39m=\u001b[39;49mresidual, output_gating\u001b[39m=\u001b[39;49mcontext\u001b[39m.\u001b[39;49moutput_adapter_gating_scores\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    287\u001b[0m hidden_states, up \u001b[39m=\u001b[39m layer_output[\u001b[39m0\u001b[39m], layer_output[\u001b[39m2\u001b[39m]\n\u001b[1;32m    288\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store_gating_score(adapter_stack_layer, layer_output[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m hook_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/peft_comparison/adapter-transformers/src/adapters/modeling.py:172\u001b[0m, in \u001b[0;36mAdapter.forward\u001b[0;34m(self, x, residual_input, output_gating)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, residual_input, output_gating\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 172\u001b[0m     down \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madapter_down(x)\n\u001b[1;32m    174\u001b[0m     up \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter_up(down)\n\u001b[1;32m    175\u001b[0m     up \u001b[39m=\u001b[39m up \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m hook_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    211\u001b[0m \u001b[39m# NB: We can't really type check this function as the type of input\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m# may change dynamically (as is tested in\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m# TestScript.test_sequential_intermediary_types).  Cannot annotate\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39m# with Any as TorchScript expects a more precise type\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m    217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m)\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[39mif\u001b[39;00m hook_id \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[1;32m   1517\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1519\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1520\u001b[0m             args, kwargs \u001b[39m=\u001b[39m result\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1523\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mforward pre-hook must return None or a tuple \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             )\n\u001b[1;32m   1526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args)\n\u001b[1;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Half and Float"
     ]
    }
   ],
   "source": [
    "model(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
