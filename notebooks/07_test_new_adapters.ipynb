{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import adapters\n",
    "\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoConfig\n",
    "from adapters import LlamaAdapterModel\n",
    "from transformers import AutoTokenizer\n",
    "from adapters.models.llama.adapter_model import LlamaAdapterModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1b\")\n",
    "\n",
    "#\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "#config.load_in_8bit = True\n",
    "\n",
    "#\n",
    "model = LlamaAdapterModel.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_4bit=True)\n",
    "model.add_causal_lm_head(\"lm_head\")\n",
    "\n",
    "#\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_4bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAdapterModel(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayerWithAdapters(\n",
       "        (self_attn): LlamaAttentionWithAdapters(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (prefix_tuning): PrefixTuningShim(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (attention_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (lm_head): CausalLMHead(\n",
       "      (0): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.heads.lm_head[0].weight = model_ref.lm_head.weight\n",
    "del model_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_ref\u001b[39m.\u001b[39mlm_head\u001b[39m.\u001b[39mweight\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ref' is not defined"
     ]
    }
   ],
   "source": [
    "model_ref.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for name, par in model.named_parameters():\n",
    "    par.requires_grad = False\n",
    "\n",
    "#model.add_adapter(\"adapter\", config=\"lora\", set_active=True)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAdapterModel(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayerWithAdapters(\n",
       "        (self_attn): LlamaAttentionWithAdapters(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (prefix_tuning): PrefixTuningShim(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "        (attention_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (lm_head): CausalLMHead(\n",
       "      (0): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdapterLayer(\n",
       "  (adapters): ModuleDict()\n",
       "  (adapter_fusion_layer): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].attention_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters.init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toked = tokenizer.encode_plus(\n",
    "    \"Sentence: I feel good. Sentiment of the previous sentence: Positive. Sentence: I feel bad. Sentiment of the previous sentence: \", return_tensors=\"pt\")\n",
    "input_ids = toked[\"input_ids\"]\n",
    "attn_mask = toked[\"attention_mask\"]\n",
    "input_ids = input_ids.to(device)\n",
    "attn_mask = attn_mask.to(device)\n",
    "print(input_ids.shape)\n",
    "print(input_ids.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_length=100,\n",
    "    min_length=2,\n",
    "    num_beams=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(\"adapter\", \"pfeiffer\", set_active=True)\n",
    "model.train()\n",
    "model.train_adapter(\"adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].output_adapters.adapters.adapter.adapter_down[0].weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model when it is dispatched on multiple devices.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/07_test_new_adapters.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/accelerate/big_modeling.py:416\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mdevice \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    415\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/transformers/modeling_utils.py:2053\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m \u001b[39m@wraps\u001b[39m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule\u001b[39m.\u001b[39mto)\n\u001b[1;32m   2050\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2051\u001b[0m     \u001b[39m# Checks if the model has been loaded in 8-bit\u001b[39;00m\n\u001b[1;32m   2052\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantization_method\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mBITS_AND_BYTES:\n\u001b[0;32m-> 2053\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2054\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2055\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2056\u001b[0m         )\n\u001b[1;32m   2057\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2058\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.attention_adapters\n",
      "model.layers.0.attention_adapters.adapters\n",
      "model.layers.0.attention_adapters.adapter_fusion_layer\n",
      "model.layers.0.output_adapters\n",
      "model.layers.0.output_adapters.adapters\n",
      "model.layers.0.output_adapters.adapters.adapter\n",
      "model.layers.0.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.0.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.0.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.0.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.0.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.0.output_adapters.adapter_fusion_layer\n",
      "model.layers.1.attention_adapters\n",
      "model.layers.1.attention_adapters.adapters\n",
      "model.layers.1.attention_adapters.adapter_fusion_layer\n",
      "model.layers.1.output_adapters\n",
      "model.layers.1.output_adapters.adapters\n",
      "model.layers.1.output_adapters.adapters.adapter\n",
      "model.layers.1.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.1.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.1.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.1.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.1.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.1.output_adapters.adapter_fusion_layer\n",
      "model.layers.2.attention_adapters\n",
      "model.layers.2.attention_adapters.adapters\n",
      "model.layers.2.attention_adapters.adapter_fusion_layer\n",
      "model.layers.2.output_adapters\n",
      "model.layers.2.output_adapters.adapters\n",
      "model.layers.2.output_adapters.adapters.adapter\n",
      "model.layers.2.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.2.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.2.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.2.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.2.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.2.output_adapters.adapter_fusion_layer\n",
      "model.layers.3.attention_adapters\n",
      "model.layers.3.attention_adapters.adapters\n",
      "model.layers.3.attention_adapters.adapter_fusion_layer\n",
      "model.layers.3.output_adapters\n",
      "model.layers.3.output_adapters.adapters\n",
      "model.layers.3.output_adapters.adapters.adapter\n",
      "model.layers.3.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.3.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.3.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.3.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.3.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.3.output_adapters.adapter_fusion_layer\n",
      "model.layers.4.attention_adapters\n",
      "model.layers.4.attention_adapters.adapters\n",
      "model.layers.4.attention_adapters.adapter_fusion_layer\n",
      "model.layers.4.output_adapters\n",
      "model.layers.4.output_adapters.adapters\n",
      "model.layers.4.output_adapters.adapters.adapter\n",
      "model.layers.4.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.4.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.4.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.4.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.4.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.4.output_adapters.adapter_fusion_layer\n",
      "model.layers.5.attention_adapters\n",
      "model.layers.5.attention_adapters.adapters\n",
      "model.layers.5.attention_adapters.adapter_fusion_layer\n",
      "model.layers.5.output_adapters\n",
      "model.layers.5.output_adapters.adapters\n",
      "model.layers.5.output_adapters.adapters.adapter\n",
      "model.layers.5.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.5.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.5.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.5.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.5.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.5.output_adapters.adapter_fusion_layer\n",
      "model.layers.6.attention_adapters\n",
      "model.layers.6.attention_adapters.adapters\n",
      "model.layers.6.attention_adapters.adapter_fusion_layer\n",
      "model.layers.6.output_adapters\n",
      "model.layers.6.output_adapters.adapters\n",
      "model.layers.6.output_adapters.adapters.adapter\n",
      "model.layers.6.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.6.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.6.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.6.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.6.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.6.output_adapters.adapter_fusion_layer\n",
      "model.layers.7.attention_adapters\n",
      "model.layers.7.attention_adapters.adapters\n",
      "model.layers.7.attention_adapters.adapter_fusion_layer\n",
      "model.layers.7.output_adapters\n",
      "model.layers.7.output_adapters.adapters\n",
      "model.layers.7.output_adapters.adapters.adapter\n",
      "model.layers.7.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.7.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.7.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.7.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.7.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.7.output_adapters.adapter_fusion_layer\n",
      "model.layers.8.attention_adapters\n",
      "model.layers.8.attention_adapters.adapters\n",
      "model.layers.8.attention_adapters.adapter_fusion_layer\n",
      "model.layers.8.output_adapters\n",
      "model.layers.8.output_adapters.adapters\n",
      "model.layers.8.output_adapters.adapters.adapter\n",
      "model.layers.8.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.8.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.8.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.8.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.8.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.8.output_adapters.adapter_fusion_layer\n",
      "model.layers.9.attention_adapters\n",
      "model.layers.9.attention_adapters.adapters\n",
      "model.layers.9.attention_adapters.adapter_fusion_layer\n",
      "model.layers.9.output_adapters\n",
      "model.layers.9.output_adapters.adapters\n",
      "model.layers.9.output_adapters.adapters.adapter\n",
      "model.layers.9.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.9.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.9.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.9.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.9.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.9.output_adapters.adapter_fusion_layer\n",
      "model.layers.10.attention_adapters\n",
      "model.layers.10.attention_adapters.adapters\n",
      "model.layers.10.attention_adapters.adapter_fusion_layer\n",
      "model.layers.10.output_adapters\n",
      "model.layers.10.output_adapters.adapters\n",
      "model.layers.10.output_adapters.adapters.adapter\n",
      "model.layers.10.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.10.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.10.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.10.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.10.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.10.output_adapters.adapter_fusion_layer\n",
      "model.layers.11.attention_adapters\n",
      "model.layers.11.attention_adapters.adapters\n",
      "model.layers.11.attention_adapters.adapter_fusion_layer\n",
      "model.layers.11.output_adapters\n",
      "model.layers.11.output_adapters.adapters\n",
      "model.layers.11.output_adapters.adapters.adapter\n",
      "model.layers.11.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.11.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.11.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.11.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.11.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.11.output_adapters.adapter_fusion_layer\n",
      "model.layers.12.attention_adapters\n",
      "model.layers.12.attention_adapters.adapters\n",
      "model.layers.12.attention_adapters.adapter_fusion_layer\n",
      "model.layers.12.output_adapters\n",
      "model.layers.12.output_adapters.adapters\n",
      "model.layers.12.output_adapters.adapters.adapter\n",
      "model.layers.12.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.12.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.12.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.12.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.12.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.12.output_adapters.adapter_fusion_layer\n",
      "model.layers.13.attention_adapters\n",
      "model.layers.13.attention_adapters.adapters\n",
      "model.layers.13.attention_adapters.adapter_fusion_layer\n",
      "model.layers.13.output_adapters\n",
      "model.layers.13.output_adapters.adapters\n",
      "model.layers.13.output_adapters.adapters.adapter\n",
      "model.layers.13.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.13.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.13.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.13.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.13.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.13.output_adapters.adapter_fusion_layer\n",
      "model.layers.14.attention_adapters\n",
      "model.layers.14.attention_adapters.adapters\n",
      "model.layers.14.attention_adapters.adapter_fusion_layer\n",
      "model.layers.14.output_adapters\n",
      "model.layers.14.output_adapters.adapters\n",
      "model.layers.14.output_adapters.adapters.adapter\n",
      "model.layers.14.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.14.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.14.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.14.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.14.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.14.output_adapters.adapter_fusion_layer\n",
      "model.layers.15.attention_adapters\n",
      "model.layers.15.attention_adapters.adapters\n",
      "model.layers.15.attention_adapters.adapter_fusion_layer\n",
      "model.layers.15.output_adapters\n",
      "model.layers.15.output_adapters.adapters\n",
      "model.layers.15.output_adapters.adapters.adapter\n",
      "model.layers.15.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.15.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.15.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.15.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.15.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.15.output_adapters.adapter_fusion_layer\n",
      "model.layers.16.attention_adapters\n",
      "model.layers.16.attention_adapters.adapters\n",
      "model.layers.16.attention_adapters.adapter_fusion_layer\n",
      "model.layers.16.output_adapters\n",
      "model.layers.16.output_adapters.adapters\n",
      "model.layers.16.output_adapters.adapters.adapter\n",
      "model.layers.16.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.16.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.16.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.16.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.16.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.16.output_adapters.adapter_fusion_layer\n",
      "model.layers.17.attention_adapters\n",
      "model.layers.17.attention_adapters.adapters\n",
      "model.layers.17.attention_adapters.adapter_fusion_layer\n",
      "model.layers.17.output_adapters\n",
      "model.layers.17.output_adapters.adapters\n",
      "model.layers.17.output_adapters.adapters.adapter\n",
      "model.layers.17.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.17.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.17.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.17.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.17.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.17.output_adapters.adapter_fusion_layer\n",
      "model.layers.18.attention_adapters\n",
      "model.layers.18.attention_adapters.adapters\n",
      "model.layers.18.attention_adapters.adapter_fusion_layer\n",
      "model.layers.18.output_adapters\n",
      "model.layers.18.output_adapters.adapters\n",
      "model.layers.18.output_adapters.adapters.adapter\n",
      "model.layers.18.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.18.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.18.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.18.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.18.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.18.output_adapters.adapter_fusion_layer\n",
      "model.layers.19.attention_adapters\n",
      "model.layers.19.attention_adapters.adapters\n",
      "model.layers.19.attention_adapters.adapter_fusion_layer\n",
      "model.layers.19.output_adapters\n",
      "model.layers.19.output_adapters.adapters\n",
      "model.layers.19.output_adapters.adapters.adapter\n",
      "model.layers.19.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.19.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.19.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.19.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.19.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.19.output_adapters.adapter_fusion_layer\n",
      "model.layers.20.attention_adapters\n",
      "model.layers.20.attention_adapters.adapters\n",
      "model.layers.20.attention_adapters.adapter_fusion_layer\n",
      "model.layers.20.output_adapters\n",
      "model.layers.20.output_adapters.adapters\n",
      "model.layers.20.output_adapters.adapters.adapter\n",
      "model.layers.20.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.20.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.20.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.20.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.20.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.20.output_adapters.adapter_fusion_layer\n",
      "model.layers.21.attention_adapters\n",
      "model.layers.21.attention_adapters.adapters\n",
      "model.layers.21.attention_adapters.adapter_fusion_layer\n",
      "model.layers.21.output_adapters\n",
      "model.layers.21.output_adapters.adapters\n",
      "model.layers.21.output_adapters.adapters.adapter\n",
      "model.layers.21.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.21.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.21.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.21.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.21.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.21.output_adapters.adapter_fusion_layer\n",
      "model.layers.22.attention_adapters\n",
      "model.layers.22.attention_adapters.adapters\n",
      "model.layers.22.attention_adapters.adapter_fusion_layer\n",
      "model.layers.22.output_adapters\n",
      "model.layers.22.output_adapters.adapters\n",
      "model.layers.22.output_adapters.adapters.adapter\n",
      "model.layers.22.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.22.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.22.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.22.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.22.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.22.output_adapters.adapter_fusion_layer\n",
      "model.layers.23.attention_adapters\n",
      "model.layers.23.attention_adapters.adapters\n",
      "model.layers.23.attention_adapters.adapter_fusion_layer\n",
      "model.layers.23.output_adapters\n",
      "model.layers.23.output_adapters.adapters\n",
      "model.layers.23.output_adapters.adapters.adapter\n",
      "model.layers.23.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.23.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.23.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.23.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.23.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.23.output_adapters.adapter_fusion_layer\n",
      "model.layers.24.attention_adapters\n",
      "model.layers.24.attention_adapters.adapters\n",
      "model.layers.24.attention_adapters.adapter_fusion_layer\n",
      "model.layers.24.output_adapters\n",
      "model.layers.24.output_adapters.adapters\n",
      "model.layers.24.output_adapters.adapters.adapter\n",
      "model.layers.24.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.24.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.24.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.24.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.24.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.24.output_adapters.adapter_fusion_layer\n",
      "model.layers.25.attention_adapters\n",
      "model.layers.25.attention_adapters.adapters\n",
      "model.layers.25.attention_adapters.adapter_fusion_layer\n",
      "model.layers.25.output_adapters\n",
      "model.layers.25.output_adapters.adapters\n",
      "model.layers.25.output_adapters.adapters.adapter\n",
      "model.layers.25.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.25.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.25.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.25.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.25.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.25.output_adapters.adapter_fusion_layer\n",
      "model.layers.26.attention_adapters\n",
      "model.layers.26.attention_adapters.adapters\n",
      "model.layers.26.attention_adapters.adapter_fusion_layer\n",
      "model.layers.26.output_adapters\n",
      "model.layers.26.output_adapters.adapters\n",
      "model.layers.26.output_adapters.adapters.adapter\n",
      "model.layers.26.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.26.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.26.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.26.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.26.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.26.output_adapters.adapter_fusion_layer\n",
      "model.layers.27.attention_adapters\n",
      "model.layers.27.attention_adapters.adapters\n",
      "model.layers.27.attention_adapters.adapter_fusion_layer\n",
      "model.layers.27.output_adapters\n",
      "model.layers.27.output_adapters.adapters\n",
      "model.layers.27.output_adapters.adapters.adapter\n",
      "model.layers.27.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.27.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.27.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.27.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.27.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.27.output_adapters.adapter_fusion_layer\n",
      "model.layers.28.attention_adapters\n",
      "model.layers.28.attention_adapters.adapters\n",
      "model.layers.28.attention_adapters.adapter_fusion_layer\n",
      "model.layers.28.output_adapters\n",
      "model.layers.28.output_adapters.adapters\n",
      "model.layers.28.output_adapters.adapters.adapter\n",
      "model.layers.28.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.28.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.28.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.28.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.28.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.28.output_adapters.adapter_fusion_layer\n",
      "model.layers.29.attention_adapters\n",
      "model.layers.29.attention_adapters.adapters\n",
      "model.layers.29.attention_adapters.adapter_fusion_layer\n",
      "model.layers.29.output_adapters\n",
      "model.layers.29.output_adapters.adapters\n",
      "model.layers.29.output_adapters.adapters.adapter\n",
      "model.layers.29.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.29.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.29.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.29.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.29.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.29.output_adapters.adapter_fusion_layer\n",
      "model.layers.30.attention_adapters\n",
      "model.layers.30.attention_adapters.adapters\n",
      "model.layers.30.attention_adapters.adapter_fusion_layer\n",
      "model.layers.30.output_adapters\n",
      "model.layers.30.output_adapters.adapters\n",
      "model.layers.30.output_adapters.adapters.adapter\n",
      "model.layers.30.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.30.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.30.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.30.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.30.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.30.output_adapters.adapter_fusion_layer\n",
      "model.layers.31.attention_adapters\n",
      "model.layers.31.attention_adapters.adapters\n",
      "model.layers.31.attention_adapters.adapter_fusion_layer\n",
      "model.layers.31.output_adapters\n",
      "model.layers.31.output_adapters.adapters\n",
      "model.layers.31.output_adapters.adapters.adapter\n",
      "model.layers.31.output_adapters.adapters.adapter.non_linearity\n",
      "model.layers.31.output_adapters.adapters.adapter.non_linearity.f\n",
      "model.layers.31.output_adapters.adapters.adapter.adapter_down\n",
      "model.layers.31.output_adapters.adapters.adapter.adapter_down.0\n",
      "model.layers.31.output_adapters.adapters.adapter.adapter_up\n",
      "model.layers.31.output_adapters.adapter_fusion_layer\n",
      "model.invertible_adapters\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"adapter\" in name:\n",
    "        module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].output_adapters.adapters.adapter.adapter_down[0].weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
