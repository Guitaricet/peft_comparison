{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pprint import pformat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm, trange\n",
    "from loguru import logger\n",
    "\n",
    "import scripts\n",
    "from adapters.models.llama.adapter_model import LlamaAdapterModel\n",
    "import peft_comparison\n",
    "import peft_comparison.text2text_utils\n",
    "import peft_comparison.mappings\n",
    "from peft_comparison.collation import DataCollatorForSeq2SeqWithMetadata, DataCollatorForCausalLMWithMetadata\n",
    "from peft_comparison.tokenization_llama_fast import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = \"max_length\"\n",
    "truncation = True\n",
    "\n",
    "source_prefix = \"\"\n",
    "max_source_length = 512\n",
    "decoder_only = True\n",
    "max_target_length = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we tokenize all the texts.\n",
    "def preprocess_function(examples, is_eval=False, decoder_only=False):\n",
    "    inputs = examples[\"source_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    inputs = [source_prefix + inp for inp in inputs]\n",
    "\n",
    "    if not decoder_only:\n",
    "        model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "        if padding == \"max_length\":\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        if is_eval:\n",
    "            model_inputs[\"metadata\"] = [{\"targets\": t} for t in targets]\n",
    "\n",
    "    else:\n",
    "        model_inputs = tokenizer(inputs, targets, max_length=max_source_length, padding=padding, truncation=True)\n",
    "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "        if is_eval:\n",
    "            input_wo_label = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=False)\n",
    "            input_wo_label = input_wo_label[\"input_ids\"]\n",
    "            model_inputs[\"metadata\"] = []\n",
    "            for idx in range(len(targets)):\n",
    "                model_inputs[\"metadata\"].append(\n",
    "                    {\n",
    "                        \"targets\": targets[idx],\n",
    "                        \"input_len\": len(input_wo_label[idx]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2490 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'passage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m raw_datasets \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39msuper_glue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrte\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m raw_datasets, postprocess_fn \u001b[39m=\u001b[39m peft_comparison\u001b[39m.\u001b[39;49mtext2text_utils\u001b[39m.\u001b[39;49mdataset_to_text2text(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     raw_datasets,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     task_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclassification\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     dataset_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mboolq\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m column_names \u001b[39m=\u001b[39m raw_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcolumn_names\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bml1/usr/cs/grad/doc/vdeshpan/peft_comparison/notebooks/08_issues_in_llama_collation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m padding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#if pad_to_max_length else False\u001b[39;00m\n",
      "File \u001b[0;32m~/peft_comparison/peft_comparison/text2text_utils.py:50\u001b[0m, in \u001b[0;36mdataset_to_text2text\u001b[0;34m(dataset, task_type, dataset_name, decoder_only)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m dataset_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m clf_label_names_mapping:\n\u001b[1;32m     48\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown dataset name: \u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m. Note that for classification, dataset_name must be a glue/superglue task name (e.g., \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcola\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     51\u001b[0m     preprocess_glue_one_example,\n\u001b[1;32m     52\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     53\u001b[0m     fn_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtask_name\u001b[39;49m\u001b[39m\"\u001b[39;49m: dataset_name, \u001b[39m\"\u001b[39;49m\u001b[39mlabel_names\u001b[39;49m\u001b[39m\"\u001b[39;49m: clf_label_names_mapping[dataset_name], \u001b[39m\"\u001b[39;49m\u001b[39mdecoder_only\u001b[39;49m\u001b[39m\"\u001b[39;49m: decoder_only}\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset, \u001b[39mdict\u001b[39m):\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m subset \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/dataset_dict.py:853\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 853\u001b[0m     {\n\u001b[1;32m    854\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    855\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    859\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    867\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    871\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    872\u001b[0m         )\n\u001b[1;32m    873\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    874\u001b[0m     }\n\u001b[1;32m    875\u001b[0m )\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/dataset_dict.py:854\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    852\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    853\u001b[0m     {\n\u001b[0;32m--> 854\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    855\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    856\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    857\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    858\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    859\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    860\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    861\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    862\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    863\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    864\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    865\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    866\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    867\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    868\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    869\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    870\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    871\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    872\u001b[0m         )\n\u001b[1;32m    873\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    874\u001b[0m     }\n\u001b[1;32m    875\u001b[0m )\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "File \u001b[0;32m~/peft_comparison/peft_comparison/text2text_utils.py:118\u001b[0m, in \u001b[0;36mpreprocess_glue_one_example\u001b[0;34m(x, task_name, label_names, feature_names, id_key, decoder_only)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mfor\u001b[39;00m feature_name \u001b[39min\u001b[39;00m feature_names:\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m feature_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     input_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfeature_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mx[feature_name]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[39m# label name\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mipdb\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/datasets/formatting/formatting.py:270\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 270\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[key]\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys_to_format:\n\u001b[1;32m    272\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'passage'"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"super_glue\", \"rte\")\n",
    "raw_datasets, postprocess_fn = peft_comparison.text2text_utils.dataset_to_text2text(\n",
    "    raw_datasets,\n",
    "    task_type=\"classification\",\n",
    "    dataset_name=\"rte\",\n",
    ")\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "padding = \"max_length\" #if pad_to_max_length else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on val dataset  \",\n",
    "    fn_kwargs={\"is_eval\": True, \"decoder_only\": decoder_only},\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=min(5000, len(raw_datasets[\"train\"]) // 8),\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    "    fn_kwargs={\"decoder_only\": decoder_only},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(eval_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(train_dataset)):\n",
    "    if train_dataset[i][\"input_ids\"].__len__() != 512:\n",
    "        print(i)\n",
    "        print(train_dataset[i][\"input_ids\"].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCausalLMWithMetadata(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=padding,\n",
    "    max_length=max_source_length,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=2)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    if batch_idx == 0:\n",
    "        \n",
    "        logger.info(\"============= CHECKING FIRST BATCH =============\")\n",
    "        logger.info(\"\\nTensor shapes: \")\n",
    "        logger.info(batch[\"input_ids\"].shape)\n",
    "\n",
    "        logger.info(\"\\nFirst example in tensor: \")\n",
    "        logger.info(batch[\"input_ids\"][0, :])\n",
    "\n",
    "        print(sum(sum(batch[\"input_ids\"] > 32000)))\n",
    "\n",
    "        logger.info(\"\\nDecoded text of first example in the batch:\")\n",
    "        s_text = tokenizer.batch_decode(batch[\"input_ids\"][0, :].unsqueeze(0), skip_special_tokens=False)\n",
    "        logger.info(f\"Source text: {s_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(eval_dataloader):\n",
    "    if batch_idx == 0:\n",
    "        \n",
    "        logger.info(\"============= CHECKING FIRST BATCH =============\")\n",
    "        logger.info(\"\\nTensor shapes: \")\n",
    "        logger.info(batch[\"input_ids\"].shape)\n",
    "\n",
    "        logger.info(\"\\nFirst example in tensor: \")\n",
    "        logger.info(batch[\"input_ids\"][0, :])\n",
    "\n",
    "        print(sum(sum(batch[\"input_ids\"] > 32000)))\n",
    "\n",
    "        logger.info(\"\\nDecoded text of first example in the batch:\")\n",
    "        s_text = tokenizer.batch_decode(batch[\"input_ids\"][0, :].unsqueeze(0), skip_special_tokens=False)\n",
    "        logger.info(f\"Source text: {s_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_comparison_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
