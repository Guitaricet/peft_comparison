{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pprint import pformat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm, trange\n",
    "from loguru import logger\n",
    "\n",
    "import scripts\n",
    "from adapters.models.llama.adapter_model import LlamaAdapterModel\n",
    "import peft_comparison\n",
    "import peft_comparison.text2text_utils\n",
    "import peft_comparison.mappings\n",
    "from peft_comparison.collation import DataCollatorForSeq2SeqWithMetadata, DataCollatorForCausalLMWithMetadata\n",
    "from peft_comparison.tokenization_llama_fast import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = \"max_length\"\n",
    "truncation = True\n",
    "\n",
    "source_prefix = \"\"\n",
    "max_source_length = 512\n",
    "decoder_only = True\n",
    "max_target_length = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, is_eval=False, decoder_only=False):\n",
    "    inputs = examples[\"source_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    inputs = [source_prefix + inp for inp in inputs]\n",
    "\n",
    "    if not decoder_only:\n",
    "        model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=truncation)\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=truncation)\n",
    "        if padding == \"max_length\":\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        if is_eval:\n",
    "            model_inputs[\"metadata\"] = [{\"targets\": t} for t in targets]\n",
    "    else:\n",
    "        model_inputs = tokenizer(inputs, targets, max_length=max_source_length, padding=padding, truncation=True)\n",
    "        if is_eval:\n",
    "            tokenized_source_text = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=False)\n",
    "            model_inputs[\"metadata\"] = [{\"input_len\": len(i)} for i in tokenized_source_text[\"input_ids\"]]\n",
    "        #model_inputs[\"labels\"] = []\n",
    "        #model_inputs[\"input_len\"] = []\n",
    "        \"\"\"\n",
    "        for idx, example in enumerate(model_inputs[\"input_ids\"]):\n",
    "            label = [tokenizer.pad_token_id] * len(example)\n",
    "            label[0:len(example)-1] = example[1:]\n",
    "            label[label == tokenizer.pad_token_id] = -100\n",
    "            model_inputs[\"labels\"].append(label)\n",
    "            #model_inputs[\"input_len\"].append(len(example))\n",
    "\n",
    "            for i in label:\n",
    "                assert type(i) == int\n",
    "            for i in example:\n",
    "                assert type(i) == int\n",
    "            assert type(label) == list\n",
    "            assert type(example) == list\n",
    "            #print(type(model_inputs[\"input_ids\"]))\n",
    "            #print(example)\n",
    "            #print(label)\n",
    "        \"\"\"\n",
    "        \n",
    "    #\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"super_glue\", \"boolq\")\n",
    "raw_datasets, postprocess_fn = peft_comparison.text2text_utils.dataset_to_text2text(\n",
    "    raw_datasets,\n",
    "    task_type=\"classification\",\n",
    "    dataset_name=\"boolq\",\n",
    ")\n",
    "column_names = list(raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on val dataset   (num_proc=8): 100%|██████████| 3270/3270 [00:00<00:00, 5571.70 examples/s]\n",
      "Running tokenizer on train dataset (num_proc=8): 100%|██████████| 9427/9427 [00:00<00:00, 9973.09 examples/s] \n"
     ]
    }
   ],
   "source": [
    "eval_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on val dataset  \",\n",
    "    fn_kwargs={\"is_eval\": True, \"decoder_only\": decoder_only},\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=min(5000, len(raw_datasets[\"train\"]) // 8),\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    "    fn_kwargs={\"decoder_only\": decoder_only}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(train_dataset)):\n",
    "    if train_dataset[i][\"input_ids\"].__len__() != 512:\n",
    "        print(i)\n",
    "        print(train_dataset[i][\"input_ids\"].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCausalLMWithMetadata(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=padding,\n",
    "    max_length=max_source_length,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=2)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-27 00:20:00.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m============= CHECKING FIRST BATCH =============\u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:00.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1m\n",
      "Tensor shapes: \u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:00.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mtorch.Size([2, 512])\u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:00.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1m\n",
      "First example in tensor: \u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:00.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mtensor([    1,  6120, 29939, 13382, 29901,   341, 24495, 10969, 29879,  1192,\n",
      "         6811,   278,  2440, 29892,   341, 24495, 16692,   278,   323,  4727,\n",
      "        29899, 29933,  2782, 29892,  8922,   575, 29892, 28618, 21542,   300,\n",
      "        29892,   322,   612,   538, 29899,  2517,  1506,  4167,   322, 29914,\n",
      "          272, 14582, 29889,  1139, 29901,   526,  3147, 29891,   289,  2782,\n",
      "          322, 13630, 13840,   300,   278,  1021,     1,  1565,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2])\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-27 00:20:00.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m\n",
      "Decoded text of first example in the batch:\u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:00.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSource text: ['<s> boolq passage: MTD Products -- Over the years, MTD acquired the Troy-Bilt, Bolens, Cub Cadet, and Yard-Man brands and/or companies. question: are troy bilt and cub cadet the same<s> true</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    if batch_idx == 0:\n",
    "        \n",
    "        logger.info(\"============= CHECKING FIRST BATCH =============\")\n",
    "        logger.info(\"\\nTensor shapes: \")\n",
    "        logger.info(batch[\"input_ids\"].shape)\n",
    "\n",
    "        logger.info(\"\\nFirst example in tensor: \")\n",
    "        logger.info(batch[\"input_ids\"][0, :])\n",
    "\n",
    "        print(sum(sum(batch[\"input_ids\"] > 32000)))\n",
    "\n",
    "        logger.info(\"\\nDecoded text of first example in the batch:\")\n",
    "        s_text = tokenizer.batch_decode(batch[\"input_ids\"][0, :].unsqueeze(0), skip_special_tokens=False)\n",
    "        logger.info(f\"Source text: {s_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-27 00:20:12.096\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1m============= CHECKING FIRST BATCH =============\u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:12.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1m\n",
      "Tensor shapes: \u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:12.099\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mtorch.Size([2, 512])\u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:12.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1m\n",
      "First example in tensor: \u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:12.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mtensor([    1,  6120, 29939, 13382, 29901, 13772,   273,   324, 26413,  1192,\n",
      "         2178,  4768,   290,   465,  5771,  1549,   472,  3203,   777,   310,\n",
      "         1438,  6576, 29901,   372,  4225,   304,   367, 21633, 29892, 16531,\n",
      "        29892,   270,  1255, 29892,  6013,   358,   287, 29892,  1320, 24455,\n",
      "        29892,   322, 12138,   287, 29889,  2178,   310,  1438,  6576,  1996,\n",
      "         7788,   322,   385, 22035, 12425, 29889,   450,  3001,  5253,   310,\n",
      "         5864,  1881,   964,   278,  1889,  9401,   304,   278,  5864,  5492,\n",
      "          491, 25535,   278,  9819, 11314,   273,   324, 26413,   338,  2998,\n",
      "          408,   278,  5864, 17346,   313,   272,  4954, 27548,  4133,   373,\n",
      "         5864,  2437,  2868, 29915,  2824,  5104,  1973, 13126,   297,   263,\n",
      "        29871, 29906, 29900, 29900, 29955,  3461,   491,  3086, 26304, 17880,\n",
      "         1298,   304,   878,   342,  2582,   363, 26343, 11314,   273,   324,\n",
      "         7371,   297,   278,  3148, 29901,   697,  5190,   310, 21983,   309,\n",
      "        29899, 29888,  2491,  5864,   338,  3734,   304,  1653, 29871, 29896,\n",
      "        29889, 29941,  5864, 10340,   515,   278,  9819, 11314,   273,   324,\n",
      "        29889,   450,  5864, 17346,   363, 26438, 29883,  1662, 11314,   273,\n",
      "          324,  7371,   297, 16078,   338,   901,  7853,   519, 29892,   411,\n",
      "          697,  5190,   310, 21983,   309, 29899, 29888,  2491,  5864,  3734,\n",
      "          304,  1653, 29871, 29947,   515,   278, 11314,   273,   324, 29889,\n",
      "        24836, 17346, 21875,   526,   451,  5948,  7371, 29892,  4550, 12727,\n",
      "         1316, 13676,   505,  1063,  5759,   393,   526, 27877,   706, 29889,\n",
      "         1152,  2777, 29892,   263,  5004, 18994, 13676,   393,  5802,   310,\n",
      "        11314,   273,   324,   515, 26438, 29883,  1662, 29892,   607,  6858,\n",
      "          263, 21881, 23622,   304,  6548,  3234,  3598, 29892,  3639,   515,\n",
      "        29871, 29947,   304, 29871, 29929, 10340,   310,  5864,   363,  1269,\n",
      "         5190,  1518,  2760, 29892,   408,  9401,   304, 26343, 29892,   607,\n",
      "          871,  3639,  1048, 29871, 29896, 29889, 29941, 29946, 10340,   310,\n",
      "        26413,  5864,   363,  1269,  5190,   310,  5864,  1518,  2760, 29889,\n",
      "          319, 29871, 29906, 29900, 29900, 29953,  3014,   310,  8046,  2292,\n",
      "        27279,  6559, 29892,  1156, 29537,   292,  4832,  5004, 11898, 29892,\n",
      "        22834,   393, 20811, 11314,   273,   324,   515, 26343,  3913,  1568,\n",
      "         3109,  5697, 12154,   398,  1135, 20811, 10489, 26496, 29889,  1139,\n",
      "        29901,   947, 11314,   273,   324,  2125,   901,  5864,  1207,   393,\n",
      "        13880,     1,  2089,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            2,     2])\u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:12.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m\n",
      "Decoded text of first example in the batch:\u001b[0m\n",
      "\u001b[32m2023-10-27 00:20:12.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mSource text: [\"<s> boolq passage: Ethanol fuel -- All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline. question: does ethanol take more energy make that produces<s> false</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\"]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(eval_dataloader):\n",
    "    if batch_idx == 0:\n",
    "        \n",
    "        logger.info(\"============= CHECKING FIRST BATCH =============\")\n",
    "        logger.info(\"\\nTensor shapes: \")\n",
    "        logger.info(batch[\"input_ids\"].shape)\n",
    "\n",
    "        logger.info(\"\\nFirst example in tensor: \")\n",
    "        logger.info(batch[\"input_ids\"][0, :])\n",
    "\n",
    "        print(sum(sum(batch[\"input_ids\"] > 32000)))\n",
    "\n",
    "        logger.info(\"\\nDecoded text of first example in the batch:\")\n",
    "        s_text = tokenizer.batch_decode(batch[\"input_ids\"][0, :].unsqueeze(0), skip_special_tokens=False)\n",
    "        logger.info(f\"Source text: {s_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_comparison_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
