{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/vdeshpan/miniconda3/envs/peft_comparison_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pprint import pformat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "import datasets\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm, trange\n",
    "from loguru import logger\n",
    "\n",
    "import scripts\n",
    "from adapters.models.llama.adapter_model import LlamaAdapterModel\n",
    "import peft_comparison\n",
    "import peft_comparison.text2text_utils\n",
    "import peft_comparison.mappings\n",
    "from peft_comparison.collation import DataCollatorForSeq2SeqWithMetadata, DataCollatorForCausalLMWithMetadata\n",
    "from peft_comparison.tokenization_llama_fast import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding = \"max_length\"\n",
    "truncation = True\n",
    "\n",
    "source_prefix = \"\"\n",
    "max_source_length = 512\n",
    "decoder_only = True\n",
    "max_target_length = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we tokenize all the texts.\n",
    "def preprocess_function(examples, is_eval=False, decoder_only=False):\n",
    "    inputs = examples[\"source_text\"]\n",
    "    targets = examples[\"target_text\"]\n",
    "    inputs = [source_prefix + inp for inp in inputs]\n",
    "\n",
    "    if not decoder_only:\n",
    "        model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "        if padding == \"max_length\":\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        if is_eval:\n",
    "            model_inputs[\"metadata\"] = [{\"targets\": t} for t in targets]\n",
    "\n",
    "    else:\n",
    "        model_inputs = tokenizer(inputs, targets, max_length=max_source_length, padding=padding, truncation=True)\n",
    "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"]\n",
    "        if is_eval:\n",
    "            input_wo_label = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=False)\n",
    "            input_wo_label = input_wo_label[\"input_ids\"]\n",
    "            model_inputs[\"metadata\"] = []\n",
    "            for idx in range(len(targets)):\n",
    "                model_inputs[\"metadata\"].append(\n",
    "                    {\n",
    "                        \"targets\": targets[idx],\n",
    "                        \"input_len\": len(input_wo_label[idx]),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"super_glue\", \"rte\")\n",
    "raw_datasets, postprocess_fn = peft_comparison.text2text_utils.dataset_to_text2text(\n",
    "    raw_datasets,\n",
    "    task_type=\"classification\",\n",
    "    dataset_name=\"rte\",\n",
    ")\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "padding = \"max_length\" #if pad_to_max_length else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'No Weapons of Mass Destruction Found in Iraq Yet.',\n",
       " 'hypothesis': 'Weapons of Mass Destruction Found in Iraq.',\n",
       " 'idx': 0,\n",
       " 'label': 1,\n",
       " 'source_text': 'rte premise: No Weapons of Mass Destruction Found in Iraq Yet. hypothesis: Weapons of Mass Destruction Found in Iraq.',\n",
       " 'target_text': 'not_entailment'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on val dataset  \",\n",
    "    fn_kwargs={\"is_eval\": True, \"decoder_only\": decoder_only},\n",
    ")\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=min(5000, len(raw_datasets[\"train\"]) // 8),\n",
    "    num_proc=8,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    "    fn_kwargs={\"decoder_only\": decoder_only},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(eval_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(train_dataset)):\n",
    "    if train_dataset[i][\"input_ids\"].__len__() != 512:\n",
    "        print(i)\n",
    "        print(train_dataset[i][\"input_ids\"].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForCausalLMWithMetadata(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=padding,\n",
    "    max_length=max_source_length,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=2)\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    if batch_idx == 0:\n",
    "        \n",
    "        logger.info(\"============= CHECKING FIRST BATCH =============\")\n",
    "        logger.info(\"\\nTensor shapes: \")\n",
    "        logger.info(batch[\"input_ids\"].shape)\n",
    "\n",
    "        logger.info(\"\\nFirst example in tensor: \")\n",
    "        logger.info(batch[\"input_ids\"][0, :])\n",
    "\n",
    "        print(sum(sum(batch[\"input_ids\"] > 32000)))\n",
    "\n",
    "        logger.info(\"\\nDecoded text of first example in the batch:\")\n",
    "        s_text = tokenizer.batch_decode(batch[\"input_ids\"][0, :].unsqueeze(0), skip_special_tokens=False)\n",
    "        logger.info(f\"Source text: {s_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(eval_dataloader):\n",
    "    if batch_idx == 0:\n",
    "        \n",
    "        logger.info(\"============= CHECKING FIRST BATCH =============\")\n",
    "        logger.info(\"\\nTensor shapes: \")\n",
    "        logger.info(batch[\"input_ids\"].shape)\n",
    "\n",
    "        logger.info(\"\\nFirst example in tensor: \")\n",
    "        logger.info(batch[\"input_ids\"][0, :])\n",
    "\n",
    "        print(sum(sum(batch[\"input_ids\"] > 32000)))\n",
    "\n",
    "        logger.info(\"\\nDecoded text of first example in the batch:\")\n",
    "        s_text = tokenizer.batch_decode(batch[\"input_ids\"][0, :].unsqueeze(0), skip_special_tokens=False)\n",
    "        logger.info(f\"Source text: {s_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_comparison_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
