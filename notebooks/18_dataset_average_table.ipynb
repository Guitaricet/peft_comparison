{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Define the specific order for adapter_config_string\n",
    "# adapter_order = ['full_tuning', 'houlsby', 'pfeiffer', 'scaled_parallel', 'ln_tuning',\n",
    "#                  'lora', 'hf_lora_all', 'hf_krona', 'compacter', 'compacter++', 'ia3',\n",
    "#                  'prefix_tuning[bottleneck_size=800,kv_size=64]|par_bn', 'prefix_tuning[kv_size=64]', 'prefix_tuning_flat[kv_size=64]', 'lora[r=8,use_gating=True]|prefix_tuning[prefix_length=10,use_gating=True,kv_size=64]|seq_bn[reduction_factor=16,use_gating=True]']\n",
    "adapter_order = ['full_tuning', 'houlsby', 'pfeiffer', 'scaled_parallel', 'ln_tuning',\n",
    "                 'lora', 'hf_lora', 'hf_lora_all', 'hf_krona', 'compacter', 'compacter++', 'ia3',\n",
    "                 'mam', 'prefix_tuning', 'prefix_tuning_flat', 'unipelt']\n",
    "adapter_order = adapter_order[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique models: 3\n",
      "Unique adapter configs: 16\n",
      "(48, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>model_name_or_path</th>\n",
       "      <th>adapter_config_string</th>\n",
       "      <th>num_gpus</th>\n",
       "      <th>total_parameters</th>\n",
       "      <th>trainable_parameters</th>\n",
       "      <th colspan=\"2\" halign=\"left\">throughput_examples_per_gpu</th>\n",
       "      <th colspan=\"2\" halign=\"left\">throughput_examples_per_gpu_test</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>full_tuning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.376681e+08</td>\n",
       "      <td>7.376681e+08</td>\n",
       "      <td>214.719403</td>\n",
       "      <td>55.264638</td>\n",
       "      <td>115.174829</td>\n",
       "      <td>59.766666</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>houlsby</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.832545e+08</td>\n",
       "      <td>1.268736e+07</td>\n",
       "      <td>138.443747</td>\n",
       "      <td>10.812014</td>\n",
       "      <td>35.710892</td>\n",
       "      <td>25.454842</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>pfeiffer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.769108e+08</td>\n",
       "      <td>6.343680e+06</td>\n",
       "      <td>122.391611</td>\n",
       "      <td>31.848107</td>\n",
       "      <td>39.611460</td>\n",
       "      <td>29.724454</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>scaled_parallel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.209726e+08</td>\n",
       "      <td>5.040542e+07</td>\n",
       "      <td>151.069752</td>\n",
       "      <td>16.356705</td>\n",
       "      <td>42.550947</td>\n",
       "      <td>34.734505</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>ln_tuning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.376681e+08</td>\n",
       "      <td>1.249280e+05</td>\n",
       "      <td>188.842627</td>\n",
       "      <td>16.314189</td>\n",
       "      <td>58.052051</td>\n",
       "      <td>53.865697</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>lora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.729265e+08</td>\n",
       "      <td>2.359296e+06</td>\n",
       "      <td>134.322818</td>\n",
       "      <td>2.925753</td>\n",
       "      <td>42.027567</td>\n",
       "      <td>36.207838</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>hf_lora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.400274e+08</td>\n",
       "      <td>2.359296e+06</td>\n",
       "      <td>152.381361</td>\n",
       "      <td>9.473810</td>\n",
       "      <td>51.150351</td>\n",
       "      <td>45.099972</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>hf_lora_all</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.463188e+08</td>\n",
       "      <td>8.650752e+06</td>\n",
       "      <td>102.774782</td>\n",
       "      <td>9.835955</td>\n",
       "      <td>41.553132</td>\n",
       "      <td>37.644594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>hf_krona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.378893e+08</td>\n",
       "      <td>2.211840e+05</td>\n",
       "      <td>138.711967</td>\n",
       "      <td>10.167677</td>\n",
       "      <td>38.443289</td>\n",
       "      <td>45.264955</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>compacter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.708714e+08</td>\n",
       "      <td>3.041920e+05</td>\n",
       "      <td>100.868252</td>\n",
       "      <td>14.462490</td>\n",
       "      <td>29.621042</td>\n",
       "      <td>26.289868</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>compacter++</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.707193e+08</td>\n",
       "      <td>1.521280e+05</td>\n",
       "      <td>125.941291</td>\n",
       "      <td>14.749740</td>\n",
       "      <td>36.346075</td>\n",
       "      <td>33.696730</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>ia3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.709112e+08</td>\n",
       "      <td>3.440640e+05</td>\n",
       "      <td>152.371379</td>\n",
       "      <td>13.948096</td>\n",
       "      <td>34.801850</td>\n",
       "      <td>28.054271</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>mam</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.416370e+08</td>\n",
       "      <td>1.710698e+08</td>\n",
       "      <td>133.650778</td>\n",
       "      <td>10.632655</td>\n",
       "      <td>27.161570</td>\n",
       "      <td>22.899528</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>prefix_tuning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.478787e+08</td>\n",
       "      <td>7.731149e+07</td>\n",
       "      <td>129.632014</td>\n",
       "      <td>32.282199</td>\n",
       "      <td>37.610855</td>\n",
       "      <td>32.214523</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>prefix_tuning_flat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.749908e+08</td>\n",
       "      <td>4.423680e+06</td>\n",
       "      <td>151.022309</td>\n",
       "      <td>16.052098</td>\n",
       "      <td>19.386755</td>\n",
       "      <td>14.096015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>unipelt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.567908e+08</td>\n",
       "      <td>8.622362e+07</td>\n",
       "      <td>74.615176</td>\n",
       "      <td>12.597486</td>\n",
       "      <td>28.501938</td>\n",
       "      <td>23.743498</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>houlsby</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.897185e+09</td>\n",
       "      <td>1.268736e+07</td>\n",
       "      <td>143.737105</td>\n",
       "      <td>10.241597</td>\n",
       "      <td>6.534746</td>\n",
       "      <td>4.582285</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>pfeiffer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.890841e+09</td>\n",
       "      <td>6.343680e+06</td>\n",
       "      <td>160.156132</td>\n",
       "      <td>7.740579</td>\n",
       "      <td>6.983099</td>\n",
       "      <td>4.720452</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>scaled_parallel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.934903e+09</td>\n",
       "      <td>5.040542e+07</td>\n",
       "      <td>154.056347</td>\n",
       "      <td>16.140492</td>\n",
       "      <td>7.460226</td>\n",
       "      <td>5.558132</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>ln_tuning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.851598e+09</td>\n",
       "      <td>1.249280e+05</td>\n",
       "      <td>213.110102</td>\n",
       "      <td>16.269847</td>\n",
       "      <td>9.353894</td>\n",
       "      <td>7.056232</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>lora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.890396e+09</td>\n",
       "      <td>5.898240e+06</td>\n",
       "      <td>128.154916</td>\n",
       "      <td>19.082357</td>\n",
       "      <td>6.769296</td>\n",
       "      <td>5.276244</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>hf_lora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.857497e+09</td>\n",
       "      <td>5.898240e+06</td>\n",
       "      <td>158.340469</td>\n",
       "      <td>11.104003</td>\n",
       "      <td>9.098203</td>\n",
       "      <td>6.805224</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>hf_lora_all</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.876764e+09</td>\n",
       "      <td>2.516582e+07</td>\n",
       "      <td>110.247071</td>\n",
       "      <td>3.842771</td>\n",
       "      <td>7.292531</td>\n",
       "      <td>5.533779</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>hf_krona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.852004e+09</td>\n",
       "      <td>4.055040e+05</td>\n",
       "      <td>130.271998</td>\n",
       "      <td>23.435611</td>\n",
       "      <td>7.759072</td>\n",
       "      <td>6.534036</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>compacter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.884802e+09</td>\n",
       "      <td>3.041920e+05</td>\n",
       "      <td>107.779281</td>\n",
       "      <td>9.818094</td>\n",
       "      <td>3.746481</td>\n",
       "      <td>2.756446</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>compacter++</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.884650e+09</td>\n",
       "      <td>1.521280e+05</td>\n",
       "      <td>129.515709</td>\n",
       "      <td>17.105013</td>\n",
       "      <td>4.717736</td>\n",
       "      <td>3.475492</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>ia3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.885874e+09</td>\n",
       "      <td>1.376256e+06</td>\n",
       "      <td>161.323438</td>\n",
       "      <td>11.811353</td>\n",
       "      <td>4.964345</td>\n",
       "      <td>3.886090</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>mam</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.409904e+09</td>\n",
       "      <td>5.254066e+08</td>\n",
       "      <td>133.003191</td>\n",
       "      <td>8.177843</td>\n",
       "      <td>4.866686</td>\n",
       "      <td>3.450720</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>prefix_tuning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.188744e+09</td>\n",
       "      <td>3.042463e+08</td>\n",
       "      <td>161.319002</td>\n",
       "      <td>21.120060</td>\n",
       "      <td>6.185008</td>\n",
       "      <td>5.303285</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>prefix_tuning_flat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.902192e+09</td>\n",
       "      <td>1.769472e+07</td>\n",
       "      <td>158.040630</td>\n",
       "      <td>10.077980</td>\n",
       "      <td>3.806423</td>\n",
       "      <td>2.742200</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>t5-3b</td>\n",
       "      <td>unipelt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.201195e+09</td>\n",
       "      <td>3.166974e+08</td>\n",
       "      <td>79.635562</td>\n",
       "      <td>7.367485</td>\n",
       "      <td>3.868532</td>\n",
       "      <td>2.918841</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>houlsby</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.135291e+10</td>\n",
       "      <td>1.268736e+07</td>\n",
       "      <td>148.237275</td>\n",
       "      <td>9.430602</td>\n",
       "      <td>2.344399</td>\n",
       "      <td>1.656527</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>pfeiffer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.134656e+10</td>\n",
       "      <td>6.343680e+06</td>\n",
       "      <td>172.156624</td>\n",
       "      <td>6.887976</td>\n",
       "      <td>1.780063</td>\n",
       "      <td>1.632044</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>scaled_parallel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.139063e+10</td>\n",
       "      <td>5.040542e+07</td>\n",
       "      <td>166.046751</td>\n",
       "      <td>12.133298</td>\n",
       "      <td>1.528978</td>\n",
       "      <td>1.456755</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>ln_tuning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.130732e+10</td>\n",
       "      <td>1.249280e+05</td>\n",
       "      <td>199.420903</td>\n",
       "      <td>21.850248</td>\n",
       "      <td>2.505577</td>\n",
       "      <td>2.463884</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>lora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.136027e+10</td>\n",
       "      <td>2.005402e+07</td>\n",
       "      <td>155.073796</td>\n",
       "      <td>3.077521</td>\n",
       "      <td>2.839457</td>\n",
       "      <td>0.815168</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>hf_lora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.132738e+10</td>\n",
       "      <td>2.005402e+07</td>\n",
       "      <td>155.894727</td>\n",
       "      <td>12.942648</td>\n",
       "      <td>2.336049</td>\n",
       "      <td>1.994039</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>hf_lora_all</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.139855e+10</td>\n",
       "      <td>9.122611e+07</td>\n",
       "      <td>109.768995</td>\n",
       "      <td>7.247691</td>\n",
       "      <td>1.980691</td>\n",
       "      <td>1.686557</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>hf_krona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.130810e+10</td>\n",
       "      <td>7.741440e+05</td>\n",
       "      <td>129.528779</td>\n",
       "      <td>18.317297</td>\n",
       "      <td>2.139872</td>\n",
       "      <td>1.844296</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>compacter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.134052e+10</td>\n",
       "      <td>3.041920e+05</td>\n",
       "      <td>110.960446</td>\n",
       "      <td>7.645048</td>\n",
       "      <td>1.068767</td>\n",
       "      <td>0.854558</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>compacter++</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.134037e+10</td>\n",
       "      <td>1.521280e+05</td>\n",
       "      <td>143.997171</td>\n",
       "      <td>7.792406</td>\n",
       "      <td>1.294440</td>\n",
       "      <td>1.070694</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>ia3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.134573e+10</td>\n",
       "      <td>5.505024e+06</td>\n",
       "      <td>166.732922</td>\n",
       "      <td>10.179502</td>\n",
       "      <td>1.374996</td>\n",
       "      <td>1.123280</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>prefix_tuning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.255221e+10</td>\n",
       "      <td>1.211985e+09</td>\n",
       "      <td>150.391086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.835809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>prefix_tuning_flat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.141100e+10</td>\n",
       "      <td>7.077888e+07</td>\n",
       "      <td>156.540838</td>\n",
       "      <td>24.033757</td>\n",
       "      <td>1.048622</td>\n",
       "      <td>0.885830</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>unipelt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.257881e+10</td>\n",
       "      <td>1.238592e+09</td>\n",
       "      <td>87.709774</td>\n",
       "      <td>1.290886</td>\n",
       "      <td>1.556308</td>\n",
       "      <td>0.448589</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>houlsby</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.135291e+10</td>\n",
       "      <td>1.268736e+07</td>\n",
       "      <td>2.974713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>pfeiffer</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.134656e+10</td>\n",
       "      <td>6.343680e+06</td>\n",
       "      <td>3.469892</td>\n",
       "      <td>0.091724</td>\n",
       "      <td>0.082943</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>t5-11b</td>\n",
       "      <td>ln_tuning</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.130732e+10</td>\n",
       "      <td>1.249280e+05</td>\n",
       "      <td>4.374290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name_or_path adapter_config_string num_gpus total_parameters  \\\n",
       "                                                                 mean   \n",
       "34           t5-large           full_tuning      1.0     7.376681e+08   \n",
       "38           t5-large               houlsby      1.0     7.832545e+08   \n",
       "43           t5-large              pfeiffer      1.0     7.769108e+08   \n",
       "46           t5-large       scaled_parallel      1.0     8.209726e+08   \n",
       "40           t5-large             ln_tuning      1.0     7.376681e+08   \n",
       "41           t5-large                  lora      1.0     7.729265e+08   \n",
       "36           t5-large               hf_lora      1.0     7.400274e+08   \n",
       "37           t5-large           hf_lora_all      1.0     7.463188e+08   \n",
       "35           t5-large              hf_krona      1.0     7.378893e+08   \n",
       "32           t5-large             compacter      1.0     7.708714e+08   \n",
       "33           t5-large           compacter++      1.0     7.707193e+08   \n",
       "39           t5-large                   ia3      1.0     7.709112e+08   \n",
       "42           t5-large                   mam      1.0     9.416370e+08   \n",
       "44           t5-large         prefix_tuning      1.0     8.478787e+08   \n",
       "45           t5-large    prefix_tuning_flat      1.0     7.749908e+08   \n",
       "47           t5-large               unipelt      1.0     8.567908e+08   \n",
       "22              t5-3b               houlsby      1.0     2.897185e+09   \n",
       "27              t5-3b              pfeiffer      1.0     2.890841e+09   \n",
       "30              t5-3b       scaled_parallel      1.0     2.934903e+09   \n",
       "24              t5-3b             ln_tuning      1.0     2.851598e+09   \n",
       "25              t5-3b                  lora      1.0     2.890396e+09   \n",
       "20              t5-3b               hf_lora      1.0     2.857497e+09   \n",
       "21              t5-3b           hf_lora_all      1.0     2.876764e+09   \n",
       "19              t5-3b              hf_krona      1.0     2.852004e+09   \n",
       "17              t5-3b             compacter      1.0     2.884802e+09   \n",
       "18              t5-3b           compacter++      1.0     2.884650e+09   \n",
       "23              t5-3b                   ia3      1.0     2.885874e+09   \n",
       "26              t5-3b                   mam      1.0     3.409904e+09   \n",
       "28              t5-3b         prefix_tuning      1.0     3.188744e+09   \n",
       "29              t5-3b    prefix_tuning_flat      1.0     2.902192e+09   \n",
       "31              t5-3b               unipelt      1.0     3.201195e+09   \n",
       "5              t5-11b               houlsby      1.0     1.135291e+10   \n",
       "11             t5-11b              pfeiffer      1.0     1.134656e+10   \n",
       "15             t5-11b       scaled_parallel      1.0     1.139063e+10   \n",
       "8              t5-11b             ln_tuning      1.0     1.130732e+10   \n",
       "10             t5-11b                  lora      1.0     1.136027e+10   \n",
       "3              t5-11b               hf_lora      1.0     1.132738e+10   \n",
       "4              t5-11b           hf_lora_all      1.0     1.139855e+10   \n",
       "2              t5-11b              hf_krona      1.0     1.130810e+10   \n",
       "0              t5-11b             compacter      1.0     1.134052e+10   \n",
       "1              t5-11b           compacter++      1.0     1.134037e+10   \n",
       "7              t5-11b                   ia3      1.0     1.134573e+10   \n",
       "13             t5-11b         prefix_tuning      1.0     1.255221e+10   \n",
       "14             t5-11b    prefix_tuning_flat      1.0     1.141100e+10   \n",
       "16             t5-11b               unipelt      1.0     1.257881e+10   \n",
       "6              t5-11b               houlsby      8.0     1.135291e+10   \n",
       "12             t5-11b              pfeiffer      8.0     1.134656e+10   \n",
       "9              t5-11b             ln_tuning      8.0     1.130732e+10   \n",
       "\n",
       "   trainable_parameters throughput_examples_per_gpu             \\\n",
       "                   mean                        mean        std   \n",
       "34         7.376681e+08                  214.719403  55.264638   \n",
       "38         1.268736e+07                  138.443747  10.812014   \n",
       "43         6.343680e+06                  122.391611  31.848107   \n",
       "46         5.040542e+07                  151.069752  16.356705   \n",
       "40         1.249280e+05                  188.842627  16.314189   \n",
       "41         2.359296e+06                  134.322818   2.925753   \n",
       "36         2.359296e+06                  152.381361   9.473810   \n",
       "37         8.650752e+06                  102.774782   9.835955   \n",
       "35         2.211840e+05                  138.711967  10.167677   \n",
       "32         3.041920e+05                  100.868252  14.462490   \n",
       "33         1.521280e+05                  125.941291  14.749740   \n",
       "39         3.440640e+05                  152.371379  13.948096   \n",
       "42         1.710698e+08                  133.650778  10.632655   \n",
       "44         7.731149e+07                  129.632014  32.282199   \n",
       "45         4.423680e+06                  151.022309  16.052098   \n",
       "47         8.622362e+07                   74.615176  12.597486   \n",
       "22         1.268736e+07                  143.737105  10.241597   \n",
       "27         6.343680e+06                  160.156132   7.740579   \n",
       "30         5.040542e+07                  154.056347  16.140492   \n",
       "24         1.249280e+05                  213.110102  16.269847   \n",
       "25         5.898240e+06                  128.154916  19.082357   \n",
       "20         5.898240e+06                  158.340469  11.104003   \n",
       "21         2.516582e+07                  110.247071   3.842771   \n",
       "19         4.055040e+05                  130.271998  23.435611   \n",
       "17         3.041920e+05                  107.779281   9.818094   \n",
       "18         1.521280e+05                  129.515709  17.105013   \n",
       "23         1.376256e+06                  161.323438  11.811353   \n",
       "26         5.254066e+08                  133.003191   8.177843   \n",
       "28         3.042463e+08                  161.319002  21.120060   \n",
       "29         1.769472e+07                  158.040630  10.077980   \n",
       "31         3.166974e+08                   79.635562   7.367485   \n",
       "5          1.268736e+07                  148.237275   9.430602   \n",
       "11         6.343680e+06                  172.156624   6.887976   \n",
       "15         5.040542e+07                  166.046751  12.133298   \n",
       "8          1.249280e+05                  199.420903  21.850248   \n",
       "10         2.005402e+07                  155.073796   3.077521   \n",
       "3          2.005402e+07                  155.894727  12.942648   \n",
       "4          9.122611e+07                  109.768995   7.247691   \n",
       "2          7.741440e+05                  129.528779  18.317297   \n",
       "0          3.041920e+05                  110.960446   7.645048   \n",
       "1          1.521280e+05                  143.997171   7.792406   \n",
       "7          5.505024e+06                  166.732922  10.179502   \n",
       "13         1.211985e+09                  150.391086        NaN   \n",
       "14         7.077888e+07                  156.540838  24.033757   \n",
       "16         1.238592e+09                   87.709774   1.290886   \n",
       "6          1.268736e+07                    2.974713        NaN   \n",
       "12         6.343680e+06                    3.469892   0.091724   \n",
       "9          1.249280e+05                    4.374290        NaN   \n",
       "\n",
       "   throughput_examples_per_gpu_test             Name  \n",
       "                               mean        std count  \n",
       "34                       115.174829  59.766666     3  \n",
       "38                        35.710892  25.454842     4  \n",
       "43                        39.611460  29.724454     4  \n",
       "46                        42.550947  34.734505     4  \n",
       "40                        58.052051  53.865697     4  \n",
       "41                        42.027567  36.207838     4  \n",
       "36                        51.150351  45.099972     4  \n",
       "37                        41.553132  37.644594     4  \n",
       "35                        38.443289  45.264955     5  \n",
       "32                        29.621042  26.289868     4  \n",
       "33                        36.346075  33.696730     4  \n",
       "39                        34.801850  28.054271     4  \n",
       "42                        27.161570  22.899528     4  \n",
       "44                        37.610855  32.214523     4  \n",
       "45                        19.386755  14.096015     4  \n",
       "47                        28.501938  23.743498     4  \n",
       "22                         6.534746   4.582285     4  \n",
       "27                         6.983099   4.720452     4  \n",
       "30                         7.460226   5.558132     4  \n",
       "24                         9.353894   7.056232     4  \n",
       "25                         6.769296   5.276244     4  \n",
       "20                         9.098203   6.805224     4  \n",
       "21                         7.292531   5.533779     4  \n",
       "19                         7.759072   6.534036     4  \n",
       "17                         3.746481   2.756446     4  \n",
       "18                         4.717736   3.475492     4  \n",
       "23                         4.964345   3.886090     4  \n",
       "26                         4.866686   3.450720     4  \n",
       "28                         6.185008   5.303285     4  \n",
       "29                         3.806423   2.742200     4  \n",
       "31                         3.868532   2.918841     4  \n",
       "5                          2.344399   1.656527     7  \n",
       "11                         1.780063   1.632044     5  \n",
       "15                         1.528978   1.456755     5  \n",
       "8                          2.505577   2.463884     4  \n",
       "10                         2.839457   0.815168     3  \n",
       "3                          2.336049   1.994039     5  \n",
       "4                          1.980691   1.686557     5  \n",
       "2                          2.139872   1.844296     5  \n",
       "0                          1.068767   0.854558     5  \n",
       "1                          1.294440   1.070694     5  \n",
       "7                          1.374996   1.123280     5  \n",
       "13                         2.835809        NaN     1  \n",
       "14                         1.048622   0.885830     5  \n",
       "16                         1.556308   0.448589     2  \n",
       "6                          0.033850        NaN     1  \n",
       "12                         0.082943   0.011062     2  \n",
       "9                          0.096042        NaN     1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../wandb_results/wandb_export_2023-12-22T05_29_37.476-05_00.csv\")\n",
    "\n",
    "relevant_columns = ['model_name_or_path', 'adapter_config_string', 'total_parameters',\n",
    "                    'trainable_parameters', 'throughput_examples', 'test/throughput_examples',\n",
    "                    'total_batch_size', 'per_device_train_batch_size', 'gradient_accumulation_steps',\n",
    "                    'dataset_name', 'dataset_config_name', 'Name']\n",
    "\n",
    "df = df[relevant_columns]\n",
    "df = df.dropna()\n",
    "# df = df[df.model_name_or_path != \"t5-11b\"]\n",
    "\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"prefix_tuning[bottleneck_size=800,kv_size=64]|par_bn\", \"mam\"\n",
    ")\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"prefix_tuning[bottleneck_size=800,kv_size=128]|par_bn\", \"mam\"\n",
    ")\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"prefix_tuning_flat[kv_size=128]\", \"prefix_tuning_flat\"\n",
    ")\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"prefix_tuning_flat[kv_size=64]\", \"prefix_tuning_flat\"\n",
    ")\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"prefix_tuning[kv_size=128]\", \"prefix_tuning\"\n",
    ")\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"prefix_tuning[kv_size=64]\", \"prefix_tuning\"\n",
    ")\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"lora[r=8,use_gating=True]|prefix_tuning[prefix_length=10,use_gating=True,kv_size=128]|seq_bn[reduction_factor=16,use_gating=True]\", \"unipelt\"\n",
    ")\n",
    "df['adapter_config_string'] = df['adapter_config_string'].replace(\n",
    "    \"lora[r=8,use_gating=True]|prefix_tuning[prefix_length=10,use_gating=True,kv_size=64]|seq_bn[reduction_factor=16,use_gating=True]\", \"unipelt\"\n",
    ")\n",
    "\n",
    "# num_gpus = total_batch_size / (per_device_train_batch_size * gradient_accumulation_steps)\n",
    "df = df.assign(num_gpus=df['total_batch_size'] / (\n",
    "        df['per_device_train_batch_size'] * df['gradient_accumulation_steps']))\n",
    "\n",
    "# throughput_examples_per_gpu = throughput_examples / num_gpus\n",
    "df = df.assign(\n",
    "    throughput_examples_per_gpu=df['throughput_examples'] / df['num_gpus'])\n",
    "\n",
    "# test/throughput_examples\n",
    "df = df.assign(\n",
    "    throughput_examples_per_gpu_test=df['test/throughput_examples'] / df['num_gpus'])\n",
    "\n",
    "table = df.groupby(\n",
    "    ['model_name_or_path', 'adapter_config_string', 'num_gpus'])\\\n",
    "    .agg({\n",
    "        'total_parameters': 'mean',\n",
    "        'trainable_parameters': 'mean',\n",
    "        'throughput_examples_per_gpu': ['mean', 'std'],\n",
    "        'throughput_examples_per_gpu_test': ['mean', 'std'],\n",
    "        'Name': 'count',\n",
    "    }).reset_index()\n",
    "\n",
    "table['adapter_config_string'] = table['adapter_config_string'].astype(\n",
    "    CategoricalDtype(categories=adapter_order, ordered=True)\n",
    ")\n",
    "table = table.sort_values(\n",
    "    by=['model_name_or_path', 'num_gpus', 'adapter_config_string'],\n",
    "    ascending=[False, True, False]\n",
    ")\n",
    "\n",
    "print(\"Unique models:\", len(table['model_name_or_path'].unique()))\n",
    "print(\"Unique adapter configs:\", len(table['adapter_config_string'].unique()))\n",
    "\n",
    "print(table.shape)\n",
    "table.to_csv('../wandb_results/throughput.csv', index=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[\"Name\"][\"count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_comparison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
